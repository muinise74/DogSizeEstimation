{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561af5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a845aeaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b8fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, demo=False):                \n",
    "        self.root = root\n",
    "        self.demo = demo\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            bboxes_labels_original = ['Dog' for _ in bboxes_original]                 \n",
    "        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64)\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        return img_original, target_original\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0382f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = './'\n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec46354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.annotations_files),len(dataset.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741e91f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f9498c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original targets:\n",
      " ({'boxes': tensor([[ 35.,  32., 281., 226.]]), 'labels': tensor([1]), 'image_id': tensor([96]), 'iscrowd': tensor([0]), 'keypoints': tensor([[[ 70.,  46.,   1.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [ 94.,  61.,   1.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [ 45.,  49.,   1.],\n",
      "         [ 88., 105.,   1.],\n",
      "         [232., 111.,   1.],\n",
      "         [132.,  97.,   1.],\n",
      "         [118., 145.,   1.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [236., 169.,   1.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [120., 200.,   1.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [269., 191.,   1.],\n",
      "         [270., 181.,   1.],\n",
      "         [115., 216.,   1.],\n",
      "         [105., 213.,   1.],\n",
      "         [260., 220.,   1.],\n",
      "         [257., 211.,   1.]]])},) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original targets:\\n\", batch[1], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1c45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints):\n",
    "    fontsize = 8\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[0]+bbox[2],bbox[1]+bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 3, (255,0,0), 1)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids20names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,0,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    image = cv2.resize(image, None, fx=2, fy=2)\n",
    "    cv2.imshow(\"Output-Keypoints\",image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6ad868",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_classes_ids20names = {0: 'L_eye', 1: 'R_eye', 2:'L_ear', 3:'R_ear', 4:'Nose', 5:'Throat', 6:'Tail', 7:'withers', 8:'L_F_elbow', 9:'R_F_elbow', 10:'L_B_elbow', 11:'R_B_elbow', 12:'L_F_knee', 13:'R_F_knee', 14:'L_B_knee', 15:'R_B_knee', 16:'L_F_paw', 17:'R_F_paw', 18:'L_B_paw', 19:'R_B_pse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b518e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19648758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keypoints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7291eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d21dcdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[70, 46],\n",
       "  [0, 0],\n",
       "  [94, 61],\n",
       "  [0, 0],\n",
       "  [45, 49],\n",
       "  [88, 105],\n",
       "  [232, 111],\n",
       "  [132, 97],\n",
       "  [118, 145],\n",
       "  [0, 0],\n",
       "  [236, 169],\n",
       "  [0, 0],\n",
       "  [120, 200],\n",
       "  [0, 0],\n",
       "  [269, 191],\n",
       "  [270, 181],\n",
       "  [115, 216],\n",
       "  [105, 213],\n",
       "  [260, 220],\n",
       "  [257, 211]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c4e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e090717",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = './train/'\n",
    "KEYPOINTS_FOLDER_TEST = './test/'\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 20)\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdf4f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/165]  eta: 0:06:32  lr: 0.000007  loss: 9.4864 (9.4864)  loss_classifier: 0.7253 (0.7253)  loss_box_reg: 0.0099 (0.0099)  loss_keypoint: 8.0575 (8.0575)  loss_objectness: 0.6887 (0.6887)  loss_rpn_box_reg: 0.0050 (0.0050)  time: 2.3784  data: 0.0050  max mem: 906\n",
      "Epoch: [0]  [164/165]  eta: 0:00:00  lr: 0.001000  loss: 7.1600 (8.1284)  loss_classifier: 0.0547 (0.1632)  loss_box_reg: 0.1148 (0.0600)  loss_keypoint: 6.9276 (7.5375)  loss_objectness: 0.0425 (0.3508)  loss_rpn_box_reg: 0.0065 (0.0170)  time: 0.2227  data: 0.0016  max mem: 1868\n",
      "Epoch: [0] Total time: 0:00:36 (0.2226 s / it)\n",
      "Epoch: [1]  [  0/165]  eta: 0:00:40  lr: 0.001000  loss: 6.5481 (6.5481)  loss_classifier: 0.1139 (0.1139)  loss_box_reg: 0.2163 (0.2163)  loss_keypoint: 6.1715 (6.1715)  loss_objectness: 0.0378 (0.0378)  loss_rpn_box_reg: 0.0086 (0.0086)  time: 0.2483  data: 0.0000  max mem: 1868\n",
      "Epoch: [1]  [164/165]  eta: 0:00:00  lr: 0.001000  loss: 6.6410 (6.7601)  loss_classifier: 0.0448 (0.0529)  loss_box_reg: 0.1119 (0.1126)  loss_keypoint: 6.4666 (6.5537)  loss_objectness: 0.0222 (0.0286)  loss_rpn_box_reg: 0.0062 (0.0123)  time: 0.2393  data: 0.0016  max mem: 1868\n",
      "Epoch: [1] Total time: 0:00:37 (0.2253 s / it)\n",
      "Epoch: [2]  [  0/165]  eta: 0:00:50  lr: 0.001000  loss: 6.3682 (6.3682)  loss_classifier: 0.0523 (0.0523)  loss_box_reg: 0.1208 (0.1208)  loss_keypoint: 6.1573 (6.1573)  loss_objectness: 0.0163 (0.0163)  loss_rpn_box_reg: 0.0215 (0.0215)  time: 0.3065  data: 0.0000  max mem: 1868\n",
      "Epoch: [2]  [164/165]  eta: 0:00:00  lr: 0.001000  loss: 6.1392 (6.0981)  loss_classifier: 0.0582 (0.0511)  loss_box_reg: 0.1093 (0.1053)  loss_keypoint: 5.9542 (5.9102)  loss_objectness: 0.0172 (0.0192)  loss_rpn_box_reg: 0.0085 (0.0123)  time: 0.2387  data: 0.0023  max mem: 1868\n",
      "Epoch: [2] Total time: 0:00:38 (0.2305 s / it)\n",
      "Epoch: [3]  [  0/165]  eta: 0:00:38  lr: 0.001000  loss: 5.5518 (5.5518)  loss_classifier: 0.0839 (0.0839)  loss_box_reg: 0.1632 (0.1632)  loss_keypoint: 5.2928 (5.2928)  loss_objectness: 0.0106 (0.0106)  loss_rpn_box_reg: 0.0013 (0.0013)  time: 0.2332  data: 0.0156  max mem: 1868\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "    lr_scheduler.step()\n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), './model/weights/keypointsrcnn_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f526673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader_test.dataset.annotations_files),len(data_loader_test.dataset.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_test = iter(data_loader_test)\n",
    "images, targets = next(iterator_test)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
